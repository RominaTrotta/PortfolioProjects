{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61c71b62",
   "metadata": {},
   "source": [
    "# Scraping Top Repositories for Topics on GitHub\n",
    "<img src=\"https://miro.medium.com/max/1400/1*TXzDa2LaEOhOpOdjMJ5blg.jpeg\" width=500 height=100>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3377dd",
   "metadata": {},
   "source": [
    "## Intro\n",
    "The objective of this analysis is to scrape data from GitHub topics page using Python and tools such as requests, BeautifulSoup and Pandas. From this page we're going to get the top repositories.\n",
    "\n",
    "Web scraping is a process of extracting data from a website (`https://github.com/` in this case) and then converting this \n",
    "unstructured data into a structured one for further insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56859b73",
   "metadata": {},
   "source": [
    "## Project Outline\n",
    "\n",
    "* We're going to scrape https://github.com/topics\n",
    "\n",
    "\n",
    "* We'll get a list of topics. For each of them, we'll get:\n",
    "    - topic title,  \n",
    "    - topic page URL,\n",
    "    - topic description\n",
    "    \n",
    "    \n",
    "* For each topic,  we'll get the top repositories\n",
    "\n",
    "   \n",
    "* For each topic,  we'll grab the repo name, username, stars and repo URL\n",
    "\n",
    "\n",
    "* For each topic we'll create a CSV file in the following format:\n",
    "\n",
    "``` \n",
    "    Repo Name,Username,Stars,Repo URL\n",
    "    three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
    "    libgdx,libgdx,18300,https://github.com/libgdx/libgdx\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ef54d2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Install / import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67467308",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ibm-watson 5.3.1 requires websocket-client==1.1.0, but you have websocket-client 0.48.0 which is incompatible.\n",
      "google-api-core 2.10.1 requires protobuf<5.0.0dev,>=3.20.1, but you have protobuf 3.19.6 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Install requests library to download web pages\n",
    "!pip install requests --quiet --upgrade\n",
    "import requests\n",
    "\n",
    "\n",
    "# Install BS to parse and extract information\n",
    "!pip install beautifulsoup4 --upgrade --quiet\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034eca8d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Scrape the list of topics from GitHub\n",
    "\n",
    "- use requests to download the page\n",
    "- use BS4 to parse and extract information\n",
    "- convert to a Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc000d64",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's write a function to download the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edde748f",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_topics_page():\n",
    "    # url to fetch topics from\n",
    "    topic_url = 'https://github.com/topics'\n",
    "    \n",
    "    # download page\n",
    "    response = requests.get(topic_url)   \n",
    "    \n",
    "    # check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "        \n",
    "    # parse using Beautiful Soup\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9602056",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we defined the function, we can get some information from this doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6bbabdd",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = get_topics_page()\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c42f97f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<p class=\"f4 color-fg-muted col-md-6 mx-auto\">Browse popular topics on GitHub.</p>,\n",
       " <h3 class=\"sr-only\" id=\"sr-footer-heading\">Footer navigation</h3>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's find some classes in order to verify the doc has the proper information\n",
    "doc.find('p'), doc.find('h3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a678cb36",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's create some helper functions to parse information from the page.\n",
    "\n",
    "In order to get the specific information we need from the page we are scrapping, we must inspect its html code in order to find the different tags we'll later use. For example, to get topic titles we need the `p` tags along with the  `f3 lh-condensed mb-0 mt-1 Link--primary` class in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be436dc1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://i.imgur.com/WY0UDyF.png\" width=600 height=100>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3e5fa5d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    \"\"\"function that retrieves topic titles once we find\n",
    "    its respective tag\"\"\"\n",
    "    \n",
    "    # find proper tag and define selector class \n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p', {'class': selection_class})\n",
    "    \n",
    "    # finally creating a list of topic titles\n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text.strip())\n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf3ea51",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So let's get the list of topic titles using `get_topic_titles()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a93d78a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D', 'Ajax', 'Algorithm', 'Amp', 'Android']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = get_topic_titles(doc)\n",
    "titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78413679",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then we defined similar functions in order to get the topic descriptions and URLs\n",
    "\n",
    "1. Topic Descriptions\n",
    "2. Topic URLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af40a0e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 1. Topic Descriptions\n",
    "def get_topic_descs(doc):\n",
    "    \n",
    "    # find proper tag and define selector class \n",
    "    desc_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_desc_tags = doc.find_all('p', {'class': desc_selector})\n",
    "    \n",
    "    # create a list of topic titles\n",
    "    topic_descs = []\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "    return topic_descs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bdec06",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's get the list of topic descriptions using `get_topic_descs()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e9736cc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D refers to the use of three-dimensional graphics, modeling, and animation in various industries.',\n",
       " 'Ajax is a technique for creating interactive web applications.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descs = get_topic_descs(doc)\n",
    "descs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab5f8406",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 2. Topic URLs\n",
    "def get_topic_urls(doc):\n",
    "    \n",
    "    # find proper tag and class \n",
    "    topic_link_tags = doc.find_all('a', {'class': 'no-underline flex-1 d-flex flex-column'})\n",
    "    \n",
    "    # create a list of topic titles\n",
    "    topic_urls = []\n",
    "    website = 'https://github.com'\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(website + tag['href'])\n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ef6d2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's get the list of topic URLs using `get_topic_urls()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "434d3d91",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://github.com/topics/3d', 'https://github.com/topics/ajax']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = get_topic_urls(doc)\n",
    "urls[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d026a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, combined these functions into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e354353",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    \n",
    "    # check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topics_url))\n",
    "        \n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    topics_dict = {\n",
    "    'title': get_topic_titles(doc), \n",
    "    'description': get_topic_descs(doc),\n",
    "    'url': get_topic_urls(doc)}\n",
    "    \n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f94d52c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3D</td>\n",
       "      <td>3D refers to the use of three-dimensional grap...</td>\n",
       "      <td>https://github.com/topics/3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ajax</td>\n",
       "      <td>Ajax is a technique for creating interactive w...</td>\n",
       "      <td>https://github.com/topics/ajax</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  title                                        description  \\\n",
       "0    3D  3D refers to the use of three-dimensional grap...   \n",
       "1  Ajax  Ajax is a technique for creating interactive w...   \n",
       "\n",
       "                              url  \n",
       "0    https://github.com/topics/3d  \n",
       "1  https://github.com/topics/ajax  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_topics().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783d4a3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Get top repositories from a topic page\n",
    "\n",
    "\n",
    "- download topics pages \n",
    "- get top repositories\n",
    "- retrieve specific info from repositories such as username, repository name, stars and repository url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a0aaa",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's write some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f751bcca",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    \n",
    "    # download page\n",
    "    response = requests.get(topic_url)   \n",
    "    \n",
    "    # check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "        \n",
    "    # parse using Beautiful Soup\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_doc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "315255c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc_3d = get_topic_page('https://github.com/topics/3d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1742caba",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this case, after we inspected the html code, we found out that the information about the repositories was under the \n",
    "`h3`, `a` and `span` tags. Specifically:\n",
    "- `h3` tag contains the repository info,\n",
    "- `a` tag contains the username, repo_name and repo_url\n",
    "- `span` tag contains the stars\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18c728dd",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step by step:\n",
    "\n",
    "# Inspect and get the h3 tag selection class\n",
    "h3_selection_class = 'f3 color-fg-muted text-normal lh-condensed'\n",
    "\n",
    "# Get repo_tags from doc created in the previous section from the specific topic page we want to get the repos from\n",
    "repo_tags = doc_3d.find_all('h3', {'class': h3_selection_class })\n",
    "\n",
    "# In a given index, get username and repo name located in the 0 and 1 index respectivelly \n",
    "a_tags = repo_tags[0].find_all('a')\n",
    "a_tags[0].text.strip(), a_tags[1].text.strip()\n",
    "\n",
    "# get repo url\n",
    "website = 'https://github.com'\n",
    "repo_url = website + a_tags[1]['href']\n",
    "\n",
    "\n",
    "# get span tags and create a function to work with data properly\n",
    "star_tags = doc_3d.find_all('span', {'class': 'Counter js-social-count'})\n",
    "\n",
    "def parse_star_count(stars_str):\n",
    "    if stars_str[-1] == 'k':\n",
    "        return int(float(stars_str[:-1])*1000)\n",
    "    return int(stars_str)\n",
    "\n",
    "# check the parse_star_count()\n",
    "# use index 0 since we've been working with data in the [0] position of the doc_3d\n",
    "parse_star_count(star_tags[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68bbeb0d",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_repo_info(h3_tag, star_tag):\n",
    "    '''function that returns all the required\n",
    "    information about a repository'''\n",
    "    \n",
    "    a_tags = h3_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = 'https://github.com' + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    return username, repo_name,stars, repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ac502fb",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mrdoob', 'three.js', 91100, 'https://github.com/mrdoob/three.js')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_repo_info(repo_tags[0], star_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8893bbc",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "    \n",
    "    # get h3 tags containing repo title, repo url and username\n",
    "    h3_selection_class = 'f3 color-fg-muted text-normal lh-condensed'\n",
    "    repo_tags = topic_doc.find_all('h3', {'class': h3_selection_class })\n",
    "    \n",
    "    # get star tags\n",
    "    star_tags = topic_doc.find_all('span', {'class': 'Counter js-social-count'})\n",
    "\n",
    "    \n",
    "    topic_repos_dict={\n",
    "        'username' : [],\n",
    "        'repo_name' : [],\n",
    "        'stars' : [],\n",
    "        'repo_url' : []    \n",
    "    }\n",
    "\n",
    "    for i in range (len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i],star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "\n",
    "    topic_repo_df = pd.DataFrame(topic_repos_dict)\n",
    "    return topic_repo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9ff572d",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>stars</th>\n",
       "      <th>repo_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mrdoob</td>\n",
       "      <td>three.js</td>\n",
       "      <td>91100</td>\n",
       "      <td>https://github.com/mrdoob/three.js</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pmndrs</td>\n",
       "      <td>react-three-fiber</td>\n",
       "      <td>22400</td>\n",
       "      <td>https://github.com/pmndrs/react-three-fiber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>libgdx</td>\n",
       "      <td>libgdx</td>\n",
       "      <td>21400</td>\n",
       "      <td>https://github.com/libgdx/libgdx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BabylonJS</td>\n",
       "      <td>Babylon.js</td>\n",
       "      <td>20400</td>\n",
       "      <td>https://github.com/BabylonJS/Babylon.js</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ssloy</td>\n",
       "      <td>tinyrenderer</td>\n",
       "      <td>16800</td>\n",
       "      <td>https://github.com/ssloy/tinyrenderer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    username          repo_name  stars  \\\n",
       "0     mrdoob           three.js  91100   \n",
       "1     pmndrs  react-three-fiber  22400   \n",
       "2     libgdx             libgdx  21400   \n",
       "3  BabylonJS         Babylon.js  20400   \n",
       "4      ssloy       tinyrenderer  16800   \n",
       "\n",
       "                                      repo_url  \n",
       "0           https://github.com/mrdoob/three.js  \n",
       "1  https://github.com/pmndrs/react-three-fiber  \n",
       "2             https://github.com/libgdx/libgdx  \n",
       "3      https://github.com/BabylonJS/Babylon.js  \n",
       "4        https://github.com/ssloy/tinyrenderer  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 5 top repositories from the 3D topic\n",
    "get_topic_repos(doc_3d).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c4cd26",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create a CSV file for each topic\n",
    "\n",
    "- each of those files  will have the following format:\n",
    "\n",
    "    `Repo Name,Username,Stars,Repo URL`\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57e6d942",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def scrape_topic(topic_url, path):\n",
    "    if os.path.exists(path):\n",
    "        print('File {} already exists. On to the next one...'.format(path))\n",
    "        return\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    topic_df.to_csv(path, index = None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "451fa400",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    print('Scraping list of topics')\n",
    "    topics_df = scrape_topics()\n",
    "    \n",
    "    os.makedirs('topic_repositories', exist_ok=True)\n",
    "    for index, row in topics_df.iterrows():             \n",
    "        print('Scraping top repositories for \"{}\"'.format(row['title']))\n",
    "        \n",
    "        # use scrape_function (remember that needs 2 args: topic_url and path)\n",
    "        scrape_topic(row['url'], 'topic_repositories/{}.csv'.format(row['title'])) \n",
    "        print('Topic {} scrapped'.format(row['title']))\n",
    "    print(\"End of scrapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3f75f29",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics\n",
      "Scraping top repositories for \"3D\"\n",
      "File topic_repositories/3D.csv already exists. On to the next one...\n",
      "Topic 3D scrapped\n",
      "Scraping top repositories for \"Ajax\"\n",
      "File topic_repositories/Ajax.csv already exists. On to the next one...\n",
      "Topic Ajax scrapped\n",
      "Scraping top repositories for \"Algorithm\"\n",
      "File topic_repositories/Algorithm.csv already exists. On to the next one...\n",
      "Topic Algorithm scrapped\n",
      "Scraping top repositories for \"Amp\"\n",
      "File topic_repositories/Amp.csv already exists. On to the next one...\n",
      "Topic Amp scrapped\n",
      "Scraping top repositories for \"Android\"\n",
      "File topic_repositories/Android.csv already exists. On to the next one...\n",
      "Topic Android scrapped\n",
      "Scraping top repositories for \"Angular\"\n",
      "File topic_repositories/Angular.csv already exists. On to the next one...\n",
      "Topic Angular scrapped\n",
      "Scraping top repositories for \"Ansible\"\n",
      "File topic_repositories/Ansible.csv already exists. On to the next one...\n",
      "Topic Ansible scrapped\n",
      "Scraping top repositories for \"API\"\n",
      "File topic_repositories/API.csv already exists. On to the next one...\n",
      "Topic API scrapped\n",
      "Scraping top repositories for \"Arduino\"\n",
      "File topic_repositories/Arduino.csv already exists. On to the next one...\n",
      "Topic Arduino scrapped\n",
      "Scraping top repositories for \"ASP.NET\"\n",
      "File topic_repositories/ASP.NET.csv already exists. On to the next one...\n",
      "Topic ASP.NET scrapped\n",
      "Scraping top repositories for \"Atom\"\n",
      "File topic_repositories/Atom.csv already exists. On to the next one...\n",
      "Topic Atom scrapped\n",
      "Scraping top repositories for \"Awesome Lists\"\n",
      "File topic_repositories/Awesome Lists.csv already exists. On to the next one...\n",
      "Topic Awesome Lists scrapped\n",
      "Scraping top repositories for \"Amazon Web Services\"\n",
      "File topic_repositories/Amazon Web Services.csv already exists. On to the next one...\n",
      "Topic Amazon Web Services scrapped\n",
      "Scraping top repositories for \"Azure\"\n",
      "File topic_repositories/Azure.csv already exists. On to the next one...\n",
      "Topic Azure scrapped\n",
      "Scraping top repositories for \"Babel\"\n",
      "File topic_repositories/Babel.csv already exists. On to the next one...\n",
      "Topic Babel scrapped\n",
      "Scraping top repositories for \"Bash\"\n",
      "File topic_repositories/Bash.csv already exists. On to the next one...\n",
      "Topic Bash scrapped\n",
      "Scraping top repositories for \"Bitcoin\"\n",
      "File topic_repositories/Bitcoin.csv already exists. On to the next one...\n",
      "Topic Bitcoin scrapped\n",
      "Scraping top repositories for \"Bootstrap\"\n",
      "File topic_repositories/Bootstrap.csv already exists. On to the next one...\n",
      "Topic Bootstrap scrapped\n",
      "Scraping top repositories for \"Bot\"\n",
      "File topic_repositories/Bot.csv already exists. On to the next one...\n",
      "Topic Bot scrapped\n",
      "Scraping top repositories for \"C\"\n",
      "File topic_repositories/C.csv already exists. On to the next one...\n",
      "Topic C scrapped\n",
      "Scraping top repositories for \"Chrome\"\n",
      "File topic_repositories/Chrome.csv already exists. On to the next one...\n",
      "Topic Chrome scrapped\n",
      "Scraping top repositories for \"Chrome extension\"\n",
      "File topic_repositories/Chrome extension.csv already exists. On to the next one...\n",
      "Topic Chrome extension scrapped\n",
      "Scraping top repositories for \"Command line interface\"\n",
      "File topic_repositories/Command line interface.csv already exists. On to the next one...\n",
      "Topic Command line interface scrapped\n",
      "Scraping top repositories for \"Clojure\"\n",
      "File topic_repositories/Clojure.csv already exists. On to the next one...\n",
      "Topic Clojure scrapped\n",
      "Scraping top repositories for \"Code quality\"\n",
      "File topic_repositories/Code quality.csv already exists. On to the next one...\n",
      "Topic Code quality scrapped\n",
      "Scraping top repositories for \"Code review\"\n",
      "File topic_repositories/Code review.csv already exists. On to the next one...\n",
      "Topic Code review scrapped\n",
      "Scraping top repositories for \"Compiler\"\n",
      "File topic_repositories/Compiler.csv already exists. On to the next one...\n",
      "Topic Compiler scrapped\n",
      "Scraping top repositories for \"Continuous integration\"\n",
      "File topic_repositories/Continuous integration.csv already exists. On to the next one...\n",
      "Topic Continuous integration scrapped\n",
      "Scraping top repositories for \"COVID-19\"\n",
      "File topic_repositories/COVID-19.csv already exists. On to the next one...\n",
      "Topic COVID-19 scrapped\n",
      "Scraping top repositories for \"C++\"\n",
      "File topic_repositories/C++.csv already exists. On to the next one...\n",
      "Topic C++ scrapped\n",
      "End of scrapping\n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b515895",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>stars</th>\n",
       "      <th>repo_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CyC2018</td>\n",
       "      <td>CS-Notes</td>\n",
       "      <td>162000</td>\n",
       "      <td>https://github.com/CyC2018/CS-Notes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>practical-tutorials</td>\n",
       "      <td>project-based-learning</td>\n",
       "      <td>92700</td>\n",
       "      <td>https://github.com/practical-tutorials/project...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>azl397985856</td>\n",
       "      <td>leetcode</td>\n",
       "      <td>50600</td>\n",
       "      <td>https://github.com/azl397985856/leetcode</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              username               repo_name   stars  \\\n",
       "0              CyC2018                CS-Notes  162000   \n",
       "1  practical-tutorials  project-based-learning   92700   \n",
       "2         azl397985856                leetcode   50600   \n",
       "\n",
       "                                            repo_url  \n",
       "0                https://github.com/CyC2018/CS-Notes  \n",
       "1  https://github.com/practical-tutorials/project...  \n",
       "2           https://github.com/azl397985856/leetcode  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read any csv file to check it was created properly\n",
    "pd.read_csv(os.getcwd() + '\\\\topic_repositories\\\\C++.csv').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a96c194",
   "metadata": {},
   "source": [
    "## Summary and References \n",
    "\n",
    "* Summary:\n",
    "    1. Downloaded and scraped topics page from GitHub\n",
    "    2. After inspecting the html code, we got a list of topic titles, topic descriptions and topic urls\n",
    "    3. Put all together in a `scrape_topics()` function that returns a dataframe\n",
    "    4. Downloaded a specific topic page and after inspecting the html code, we retrieved `username, repo_name,stars, repo_url`\n",
    "    5. Put all that info together in the `get_topic_repos()` function\n",
    "    6. Created csv files of every top repository in the first page of `https://github.com/topics`\n",
    "    \n",
    "    \n",
    "* References to useful links:\n",
    "    - https://stackoverflow.com/\n",
    "    - https://pypi.org/project/beautifulsoup4/\n",
    "    - https://towardsdatascience.com/\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
